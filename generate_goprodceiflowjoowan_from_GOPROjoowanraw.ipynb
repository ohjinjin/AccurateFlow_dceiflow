{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/dcei_flow/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GOPR0396_11_00', 'GOPR0384_11_00', 'GOPR0868_11_00', 'GOPR0410_11_00', 'GOPR0385_11_01', 'GOPR0862_11_00', 'GOPR0384_11_05', 'GOPR0881_11_01', 'GOPR0871_11_00', 'GOPR0869_11_00', 'GOPR0854_11_00']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision.models.optical_flow import raft_large\n",
    "import os\n",
    "import shutil\n",
    "# from utils.file_io import read_event_h5\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# def read_event_h5(file):\n",
    "# #     file = h5py.File(path, 'r')\n",
    "#     length = len(file['x'])\n",
    "#     events = np.zeros([length, 4], dtype=np.float32)\n",
    "#     events[:, 0] = file['x']\n",
    "#     events[:, 1] = file['y']\n",
    "#     events[:, 2] = file['t']\n",
    "#     events[:, 3] = file['p']\n",
    "#     file.close()\n",
    "#     return events\n",
    "\n",
    "def write_flo(flow, filename):\n",
    "    \"\"\"\n",
    "    write optical flow in Middlebury .flo format\n",
    "    :param flow: optical flow map\n",
    "    :param filename: optical flow file path to be saved\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "#     flow = flow[0, :, :, :]\n",
    "    flow_np = flow.detach().cpu().numpy()\n",
    "    f = open(filename, 'wb')\n",
    "    magic = np.array([202021.25], dtype=np.float32)\n",
    "    height, width = flow_np.shape[:2]\n",
    "    magic.tofile(f)\n",
    "    np.int32(width).tofile(f)\n",
    "    np.int32(height).tofile(f)\n",
    "    data = np.float32(flow_np).flatten()\n",
    "    data.tofile(f)\n",
    "    f.close() \n",
    "    \n",
    "\n",
    "def preprocess(batch):\n",
    "    transforms = T.Compose(\n",
    "        [\n",
    "            T.ConvertImageDtype(torch.float32),\n",
    "            T.Normalize(mean=0.5, std=0.5),  # map [0, 1] into [-1, 1]\n",
    "#             T.Resize(size=(520, 960)),\n",
    "        ]\n",
    "    )\n",
    "    batch = transforms(batch)\n",
    "    return batch\n",
    "\n",
    "phase = 'test'\n",
    "h5_directory = f'/data/GOPRO_joowan_raw/{phase}'\n",
    "ref_directory = '/data/gopro_dceiflow_prev'\n",
    "output_directory = f'/data/gopro_dceiflow_joowan/{phase}'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "scene_names = [filename[:-3] for filename in os.listdir(h5_directory) if filename.endswith('.h5')]\n",
    "\n",
    "# print(h5_files)\n",
    "\n",
    "# # 파일 이름만 리스트에 담음\n",
    "# h5_filenames = [os.path.basename(file) for file in h5_files]\n",
    "print(scene_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = '/data/gopro_dceiflow_prev/events_train/GOPR0384_11_03/000090-event.h5'\n",
    "# with h5py.File(tmp, 'r') as tmpfile:\n",
    "#     print(tmpfile.keys())\n",
    "#     tmparr = np.array(tmpfile['t'])\n",
    "#     print(tmparr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['events', 'images', 'sharp_images', 'sharp_images_next', 'sharp_images_prev', 'synthesized_images']>\n"
     ]
    }
   ],
   "source": [
    "# d1 = '/data1/ohjinjin/nas_ohjinjin/GOPRO_EFNet_original/GOPRO/train'\n",
    "# d2 = '/data1/ohjinjin/nas_ohjinjin/GOPRO_raw_events/GOPRO_rawevents/train'\n",
    "with h5py.File(os.path.join('/data/GOPRO_joowan_raw/train', 'GOPR0384_11_03.h5'), 'r') as tmpfile:\n",
    "    print(tmpfile.keys())\n",
    "#     tmparr = np.array(tmpfile['t'])\n",
    "#     print(tmparr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def binary_search_h5_dset(dset, x, l=None, r=None, side='left'):\n",
    "#     \"\"\"\n",
    "#     Binary search for a timestamp in an HDF5 event file, without\n",
    "#     loading the entire file into RAM\n",
    "#     @param dset The HDF5 dataset\n",
    "#     @param x The timestamp being searched for\n",
    "#     @param l Starting guess for the left side (0 if None is chosen)\n",
    "#     @param r Starting guess for the right side (-1 if None is chosen)\n",
    "#     @param side Which side to take final result for if exact match is not found\n",
    "#     @returns Index of nearest event to 'x'\n",
    "#     \"\"\"\n",
    "#     l = 0 if l is None else l\n",
    "#     r = len(dset)-1 if r is None else r\n",
    "#     while l <= r:\n",
    "#         mid = l + (r - l)//2;\n",
    "#         midval = dset[mid]\n",
    "#         if midval == x:\n",
    "#             return mid\n",
    "#         elif midval < x:\n",
    "#             l = mid + 1\n",
    "#         else:\n",
    "#             r = mid - 1\n",
    "#     if side == 'left':\n",
    "#         return l\n",
    "#     return r\n",
    "\n",
    "# def find_ts_index(timestamp):\n",
    "#     idx = binary_search_h5_dset(h5_file['events/ts'], timestamp)\n",
    "#     return idx\n",
    "\n",
    "# def compute_frame_center_indeices():\n",
    "#     \"\"\"\n",
    "#     For each frame, find the start and end indices of the events around the\n",
    "#     frame, the start and the end are at the middle between the frame and the \n",
    "#     neighborhood frames\n",
    "#     \"\"\"\n",
    "#     frame_indices = []\n",
    "#     start_idx = find_ts_index((frame_ts[0]+frame_ts[1])/2)\n",
    "#     for i in range(1, len(frame_ts)-1): \n",
    "#         end_idx = find_ts_index((frame_ts[i]+frame_ts[i+1])/2)\n",
    "#         frame_indices.append([start_idx, end_idx])\n",
    "#         start_idx = end_idx\n",
    "#     return frame_indices\n",
    "\n",
    "# h5_file = file\n",
    "\n",
    "# frame_ts = []\n",
    "# for img_name in h5_file['images']:\n",
    "#     frame_ts.append(h5_file['images/{}'.format(img_name)].attrs['timestamp'])\n",
    "# event_indices = compute_frame_center_indeices()\n",
    "\n",
    "\n",
    "\n",
    "# def get_event_indices(self, index):\n",
    "#         \"\"\"\n",
    "#         Get start and end indices of events at index\n",
    "#         @param Desired data index\n",
    "#         @returns Start and end indices of events at index\n",
    "#         \"\"\"\n",
    "#         idx0, idx1 = self.event_indices[index]\n",
    "#         if not (idx0 >= 0 and idx1 <= self.num_events):\n",
    "#             raise Exception(\"WARNING: Event indices {},{} out of bounds 0,{}\".format(idx0, idx1, self.num_events))\n",
    "#         return int(idx0), int(idx1)\n",
    "\n",
    "# def get_events(self, idx0, idx1):\n",
    "#     xs = self.h5_file['events/xs'][idx0:idx1]\n",
    "#     ys = self.h5_file['events/ys'][idx0:idx1]\n",
    "#     ts = self.h5_file['events/ts'][idx0:idx1]\n",
    "#     ps = self.h5_file['events/ps'][idx0:idx1] * 2.0 - 1.0  # -1 and 1\n",
    "#     return xs, ys, ts, ps\n",
    "\n",
    "# def __getitem__(self, index, seed=None):\n",
    "#     \"\"\"\n",
    "#     Get data at index.\n",
    "#     @param index Index of data\n",
    "#     @param seed Random seed for data augmentation 用于random crop等\n",
    "#     @returns Dict with desired outputs (voxel grid, events, frames etc)\n",
    "#         as set in constructor\n",
    "#     \"\"\"\n",
    "#     if index < 0 or index >= self.__len__():\n",
    "#         raise IndexError\n",
    "#     seed = random.randint(0, 2 ** 32) if seed is None else seed\n",
    "\n",
    "#     idx0, idx1 = self.get_event_indices(index) # the start and end index of the selected events\n",
    "#     # print('DEBUG: idx0:{}, idx1:{}'.format(idx0, idx1))\n",
    "#     xs, ys, ts, ps = self.get_events(idx0, idx1) # the selected events, determined by the voxel method\n",
    "#     xs, ys, ts, ps = self.preprocess_events(xs, ys, ts, ps)\n",
    "#     ts_0, ts_k  = ts[0], ts[-1]\n",
    "#     dt = ts_k-ts_0\n",
    "\n",
    "#     item = {'data_source_idx': self.data_source_idx, 'data_path': self.data_path,\n",
    "#             'timestamp': ts_k, 'dt_between_frames': dt, 'ts_idx0': ts_0, 'ts_idx1': ts_k,\n",
    "#             'idx0': idx0, 'idx1': idx1}\n",
    "#     xs = torch.from_numpy(xs.astype(np.float32))\n",
    "#     ys = torch.from_numpy(ys.astype(np.float32))\n",
    "#     # ts = torch.from_numpy((ts-ts_0).astype(np.float32)) # ts start from 0\n",
    "#     ts = torch.from_numpy(ts.astype(np.float32)) # !\n",
    "#     ps = torch.from_numpy(ps.astype(np.float32))\n",
    "\n",
    "#     num_event_frame = list(xs.shape)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current scene is GOPR0396_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/dcei_flow/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/dcei_flow/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Raft_Large_Weights.C_T_SKHT_V2`. You can also use `weights=Raft_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current scene is GOPR0384_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 100\n",
      "Current scene is GOPR0868_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 100\n",
      "Current scene is GOPR0410_11_00\n",
      "가장 작은 숫자: 101\n",
      "가장 큰 숫자: 234\n",
      "Current scene is GOPR0385_11_01\n",
      "가장 작은 숫자: 3011\n",
      "가장 큰 숫자: 3110\n",
      "Current scene is GOPR0862_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 77\n",
      "Current scene is GOPR0384_11_05\n",
      "가장 작은 숫자: 4001\n",
      "가장 큰 숫자: 4100\n",
      "Current scene is GOPR0881_11_01\n",
      "가장 작은 숫자: 201\n",
      "가장 큰 숫자: 300\n",
      "Current scene is GOPR0871_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 100\n",
      "Current scene is GOPR0869_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 100\n",
      "Current scene is GOPR0854_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 100\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "for scene in scene_names:\n",
    "    print(f\"Current scene is {scene}\")\n",
    "    h5_file_path = os.path.join(h5_directory, scene+\".h5\")\n",
    "    with h5py.File(h5_file_path, 'r') as file:\n",
    "#         print(file.keys())\n",
    "#         data_idx = file['images'].keys()\n",
    "        \n",
    "        png_files = [pngfile for pngfile in os.listdir(os.path.join(ref_directory, phase, scene,'sharp')) if pngfile.endswith('.png')]\n",
    "        sorted_png_files = sorted(png_files, key=lambda x: int(os.path.splitext(x)[0]))\n",
    "        smallest_number = int(os.path.splitext(sorted_png_files[0])[0])\n",
    "        largest_number = int(os.path.splitext(sorted_png_files[-1])[0])\n",
    "\n",
    "        print(\"가장 작은 숫자:\", smallest_number)\n",
    "        print(\"가장 큰 숫자:\", largest_number)\n",
    "        #     print(smallest_number+len(sorted_png_files)-1)\n",
    "\n",
    "        for i in range(smallest_number,largest_number-1):\n",
    "#             image_path_1 = f\"{ref_directory}/{phase}/{scene}/sharp/{i:06d}.png\"\n",
    "#             image_path_2 = f\"{ref_directory}/{phase}/{scene}/sharp/{i + 2:06d}.png\"\n",
    "#             blur_image_path = f\"{ref_directory}/{phase}/{scene}/blur/{i + 1:06d}.png\"\n",
    "#             sharp_image_path = f\"{ref_directory}/{phase}/{scene}/sharp/{i + 1:06d}.png\"\n",
    "#             image_1 = np.array(Image.open(image_path_1))\n",
    "#             image_2 = np.array(Image.open(image_path_2))\n",
    "#             blur_image = np.array(Image.open(blur_image_path))\n",
    "#             sharp_image = np.array(Image.open(sharp_image_path))\n",
    "            \n",
    "#             plt.title(f'blur {blur_image.shape}')\n",
    "#             plt.imshow(blur_image)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp {sharp_image.shape}')\n",
    "#             plt.imshow(sharp_image)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp_prev {image_1.shape}')\n",
    "#             plt.imshow(image_1)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp_next {image_2.shape}')\n",
    "#             plt.imshow(image_2)\n",
    "#             plt.show()\n",
    "\n",
    "#         for each in data_idx:\n",
    "            blur = np.transpose(np.array(file[f'images/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            syn_blur = np.transpose(np.array(file[f'synthesized_images/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            sharp = np.transpose(np.array(file[f'sharp_images/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            sharp_prev = np.transpose(np.array(file[f'sharp_images_prev/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            sharp_next = np.transpose(np.array(file[f'sharp_images_next/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            events = np.array(file[f'events/event{i-smallest_number:09d}'])################\n",
    "#             plt.title(f'blur {blur.shape}')\n",
    "#             plt.imshow(blur)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'syn_blur {syn_blur.shape}')\n",
    "#             plt.imshow(syn_blur)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp {sharp.shape}')\n",
    "#             plt.imshow(sharp)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp_prev {sharp_prev.shape}')\n",
    "#             plt.imshow(sharp_prev)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp_next {sharp_next.shape}')\n",
    "#             plt.imshow(sharp_next)\n",
    "#             plt.show()\n",
    "            \n",
    "#             print(np.array_equal(blur, blur_image))\n",
    "#             print(np.array_equal(sharp, sharp_image))\n",
    "#             print(np.array_equal(image_1, sharp_prev))\n",
    "#             print(np.array_equal(image_2, sharp_next))\n",
    "            \n",
    "            # opticalflow estimation\n",
    "            tensor_image_1 = F.to_tensor(sharp_prev)\n",
    "            tensor_image_2 = F.to_tensor(sharp_next)\n",
    "            img1_batch = tensor_image_1.unsqueeze(0)\n",
    "            img2_batch = tensor_image_2.unsqueeze(0)\n",
    "            \n",
    "            output_blur_image_path = os.path.join(output_directory, phase, f\"{scene}_{i+1:06d}-img_0.png\")\n",
    "            output_sharp_image_path = os.path.join(output_directory, phase, \"sharp\", f\"{scene}_{i+1:06d}.png\")\n",
    "            output_second_image_path = os.path.join(output_directory, phase, f\"{scene}_{i+1:06d}-img_1.png\")\n",
    "            output_flow01_path = os.path.join(output_directory, phase, f\"{scene}_{i+1:06d}-flow_01.flo\")\n",
    "            output_flow10_path = os.path.join(output_directory, phase, f\"{scene}_{i+1:06d}-flow_10.flo\")\n",
    "            output_event_path = os.path.join(output_directory, \"events_\"+phase, f\"{scene}_{i+1:06d}-event.h5\")\n",
    "            if not os.path.exists(os.path.join(output_directory, phase)):\n",
    "                os.makedirs(os.path.join(output_directory, phase))\n",
    "            if not os.path.exists(os.path.join(output_directory, phase, \"sharp\")):\n",
    "                os.makedirs(os.path.join(output_directory, phase, \"sharp\"))\n",
    "            if not os.path.exists(os.path.join(output_directory, \"events_\"+phase)):\n",
    "                os.makedirs(os.path.join(output_directory, \"events_\"+phase))\n",
    "            \n",
    "            # NumPy 배열을 PIL 이미지로 변환 후 png 파일로 저장..\n",
    "            Image.fromarray(syn_blur.astype(np.uint8)).save(output_blur_image_path)\n",
    "            Image.fromarray(sharp.astype(np.uint8)).save(output_sharp_image_path)\n",
    "            Image.fromarray(sharp_next.astype(np.uint8)).save(output_second_image_path)\n",
    "\n",
    "            # 누적 이벤트 .h5 저장\n",
    "            \n",
    "            # HDF5 파일로 저장\n",
    "            with h5py.File(output_event_path, 'w') as ev_f:\n",
    "                ev_f.create_dataset('x', data=events[:, 0])\n",
    "                ev_f.create_dataset('y', data=events[:, 1])\n",
    "                ev_f.create_dataset('t', data=events[:, 2])\n",
    "                ev_f.create_dataset('p', data=events[:, 3])\n",
    "            \n",
    "            # If you can, run this example on a GPU, it will be a lot faster.\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            img1_batch = preprocess(img1_batch).to(device)\n",
    "            img2_batch = preprocess(img2_batch).to(device)\n",
    "\n",
    "            model = raft_large(pretrained=True, progress=False).to(device)\n",
    "            model = model.eval()\n",
    "\n",
    "            list_of_flows = model(img1_batch.to(device), img2_batch.to(device))\n",
    "            predicted_flows = list_of_flows[-1]\n",
    "            # .flo 파일로 저장\n",
    "            write_flo(predicted_flows[0].permute(1, 2, 0), output_flow01_path)\n",
    "            list_of_flows = model(img2_batch.to(device), img1_batch.to(device))\n",
    "            predicted_flows = list_of_flows[-1]\n",
    "\n",
    "            # .flo 파일로 저장\n",
    "            write_flo(predicted_flows[0].permute(1, 2, 0), output_flow10_path)\n",
    "            \n",
    "#             break\n",
    "#         break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GOPR0372_07_01', 'GOPR0384_11_02', 'GOPR0384_11_01', 'GOPR0380_11_00', 'GOPR0372_07_00', 'GOPR0477_11_00', 'GOPR0881_11_00', 'GOPR0871_11_01', 'GOPR0384_11_04', 'GOPR0374_11_01', 'GOPR0386_11_00', 'GOPR0868_11_01', 'GOPR0374_11_00', 'GOPR0857_11_00', 'GOPR0384_11_03', 'GOPR0385_11_00', 'GOPR0374_11_03', 'GOPR0868_11_02', 'GOPR0378_13_00', 'GOPR0374_11_02', 'GOPR0884_11_00', 'GOPR0379_11_00']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "phase = 'train'\n",
    "h5_directory = f'/data/GOPRO_joowan_raw/{phase}'\n",
    "ref_directory = '/data/gopro_dceiflow_prev'\n",
    "output_directory = f'/data/gopro_dceiflow_joowan/{phase}'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "scene_names = [filename[:-3] for filename in os.listdir(h5_directory) if filename.endswith('.h5')]\n",
    "\n",
    "# print(h5_files)\n",
    "\n",
    "# # 파일 이름만 리스트에 담음\n",
    "# h5_filenames = [os.path.basename(file) for file in h5_files]\n",
    "print(scene_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current scene is GOPR0372_07_01\n",
      "가장 작은 숫자: 601\n",
      "가장 큰 숫자: 675\n",
      "Current scene is GOPR0384_11_02\n",
      "가장 작은 숫자: 1301\n",
      "가장 큰 숫자: 1400\n",
      "Current scene is GOPR0384_11_01\n",
      "가장 작은 숫자: 951\n",
      "가장 큰 숫자: 1050\n",
      "Current scene is GOPR0380_11_00\n",
      "가장 작은 숫자: 134\n",
      "가장 큰 숫자: 193\n",
      "Current scene is GOPR0372_07_00\n",
      "가장 작은 숫자: 47\n",
      "가장 큰 숫자: 146\n",
      "Current scene is GOPR0477_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 80\n",
      "Current scene is GOPR0881_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 100\n",
      "Current scene is GOPR0871_11_01\n",
      "가장 작은 숫자: 181\n",
      "가장 큰 숫자: 280\n",
      "Current scene is GOPR0384_11_04\n",
      "가장 작은 숫자: 2801\n",
      "가장 큰 숫자: 2900\n",
      "Current scene is GOPR0374_11_01\n",
      "가장 작은 숫자: 203\n",
      "가장 큰 숫자: 282\n",
      "Current scene is GOPR0386_11_00\n",
      "가장 작은 숫자: 247\n",
      "가장 큰 숫자: 346\n",
      "Current scene is GOPR0868_11_01\n",
      "가장 작은 숫자: 221\n",
      "가장 큰 숫자: 320\n",
      "Current scene is GOPR0374_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 150\n",
      "Current scene is GOPR0857_11_00\n",
      "가장 작은 숫자: 1\n",
      "가장 큰 숫자: 100\n",
      "Current scene is GOPR0384_11_03\n",
      "가장 작은 숫자: 2101\n",
      "가장 큰 숫자: 2200\n",
      "Current scene is GOPR0385_11_00\n",
      "가장 작은 숫자: 101\n",
      "가장 큰 숫자: 200\n",
      "Current scene is GOPR0374_11_03\n",
      "가장 작은 숫자: 2481\n",
      "가장 큰 숫자: 2528\n",
      "Current scene is GOPR0868_11_02\n",
      "가장 작은 숫자: 681\n",
      "가장 큰 숫자: 780\n",
      "Current scene is GOPR0378_13_00\n",
      "가장 작은 숫자: 41\n",
      "가장 큰 숫자: 150\n",
      "Current scene is GOPR0374_11_02\n",
      "가장 작은 숫자: 541\n",
      "가장 큰 숫자: 640\n",
      "Current scene is GOPR0884_11_00\n",
      "가장 작은 숫자: 186\n",
      "가장 큰 숫자: 285\n",
      "Current scene is GOPR0379_11_00\n",
      "가장 작은 숫자: 188\n",
      "가장 큰 숫자: 287\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "for scene in scene_names:\n",
    "    print(f\"Current scene is {scene}\")\n",
    "    h5_file_path = os.path.join(h5_directory, scene+\".h5\")\n",
    "    with h5py.File(h5_file_path, 'r') as file:\n",
    "#         print(file.keys())\n",
    "#         data_idx = file['images'].keys()\n",
    "        \n",
    "        png_files = [pngfile for pngfile in os.listdir(os.path.join(ref_directory, phase, scene,'sharp')) if pngfile.endswith('.png')]\n",
    "        sorted_png_files = sorted(png_files, key=lambda x: int(os.path.splitext(x)[0]))\n",
    "        smallest_number = int(os.path.splitext(sorted_png_files[0])[0])\n",
    "        largest_number = int(os.path.splitext(sorted_png_files[-1])[0])\n",
    "\n",
    "        print(\"가장 작은 숫자:\", smallest_number)\n",
    "        print(\"가장 큰 숫자:\", largest_number)\n",
    "        #     print(smallest_number+len(sorted_png_files)-1)\n",
    "\n",
    "        for i in range(smallest_number,largest_number-1):\n",
    "#             image_path_1 = f\"{ref_directory}/{phase}/{scene}/sharp/{i:06d}.png\"\n",
    "#             image_path_2 = f\"{ref_directory}/{phase}/{scene}/sharp/{i + 2:06d}.png\"\n",
    "#             blur_image_path = f\"{ref_directory}/{phase}/{scene}/blur/{i + 1:06d}.png\"\n",
    "#             sharp_image_path = f\"{ref_directory}/{phase}/{scene}/sharp/{i + 1:06d}.png\"\n",
    "#             image_1 = np.array(Image.open(image_path_1))\n",
    "#             image_2 = np.array(Image.open(image_path_2))\n",
    "#             blur_image = np.array(Image.open(blur_image_path))\n",
    "#             sharp_image = np.array(Image.open(sharp_image_path))\n",
    "            \n",
    "#             plt.title(f'blur {blur_image.shape}')\n",
    "#             plt.imshow(blur_image)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp {sharp_image.shape}')\n",
    "#             plt.imshow(sharp_image)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp_prev {image_1.shape}')\n",
    "#             plt.imshow(image_1)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp_next {image_2.shape}')\n",
    "#             plt.imshow(image_2)\n",
    "#             plt.show()\n",
    "\n",
    "#         for each in data_idx:\n",
    "            blur = np.transpose(np.array(file[f'images/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            syn_blur = np.transpose(np.array(file[f'synthesized_images/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            sharp = np.transpose(np.array(file[f'sharp_images/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            sharp_prev = np.transpose(np.array(file[f'sharp_images_prev/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            sharp_next = np.transpose(np.array(file[f'sharp_images_next/image{i-smallest_number:09d}']),(1,2,0))[:, :, [2, 1, 0]]\n",
    "            events = np.array(file[f'events/event{i-smallest_number:09d}'])################\n",
    "#             plt.title(f'blur {blur.shape}')\n",
    "#             plt.imshow(blur)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'syn_blur {syn_blur.shape}')\n",
    "#             plt.imshow(syn_blur)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp {sharp.shape}')\n",
    "#             plt.imshow(sharp)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp_prev {sharp_prev.shape}')\n",
    "#             plt.imshow(sharp_prev)\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.title(f'sharp_next {sharp_next.shape}')\n",
    "#             plt.imshow(sharp_next)\n",
    "#             plt.show()\n",
    "            \n",
    "#             print(np.array_equal(blur, blur_image))\n",
    "#             print(np.array_equal(sharp, sharp_image))\n",
    "#             print(np.array_equal(image_1, sharp_prev))\n",
    "#             print(np.array_equal(image_2, sharp_next))\n",
    "            \n",
    "            # opticalflow estimation\n",
    "            tensor_image_1 = F.to_tensor(sharp_prev)\n",
    "            tensor_image_2 = F.to_tensor(sharp_next)\n",
    "            img1_batch = tensor_image_1.unsqueeze(0)\n",
    "            img2_batch = tensor_image_2.unsqueeze(0)\n",
    "            \n",
    "            output_blur_image_path = os.path.join(output_directory, phase, f\"{scene}_{i+1:06d}-img_0.png\")\n",
    "            output_sharp_image_path = os.path.join(output_directory, phase, \"sharp\", f\"{scene}_{i+1:06d}.png\")\n",
    "            output_second_image_path = os.path.join(output_directory, phase, f\"{scene}_{i+1:06d}-img_1.png\")\n",
    "            output_flow01_path = os.path.join(output_directory, phase, f\"{scene}_{i+1:06d}-flow_01.flo\")\n",
    "            output_flow10_path = os.path.join(output_directory, phase, f\"{scene}_{i+1:06d}-flow_10.flo\")\n",
    "            output_event_path = os.path.join(output_directory, \"events_\"+phase, f\"{scene}_{i+1:06d}-event.h5\")\n",
    "            if not os.path.exists(os.path.join(output_directory, phase)):\n",
    "                os.makedirs(os.path.join(output_directory, phase))\n",
    "            if not os.path.exists(os.path.join(output_directory, phase, \"sharp\")):\n",
    "                os.makedirs(os.path.join(output_directory, phase, \"sharp\"))\n",
    "            if not os.path.exists(os.path.join(output_directory, \"events_\"+phase)):\n",
    "                os.makedirs(os.path.join(output_directory, \"events_\"+phase))\n",
    "            \n",
    "            # NumPy 배열을 PIL 이미지로 변환 후 png 파일로 저장..\n",
    "            Image.fromarray(syn_blur.astype(np.uint8)).save(output_blur_image_path)\n",
    "            Image.fromarray(sharp.astype(np.uint8)).save(output_sharp_image_path)\n",
    "            Image.fromarray(sharp_next.astype(np.uint8)).save(output_second_image_path)\n",
    "\n",
    "            # 누적 이벤트 .h5 저장\n",
    "            \n",
    "            # HDF5 파일로 저장\n",
    "            with h5py.File(output_event_path, 'w') as ev_f:\n",
    "                ev_f.create_dataset('x', data=events[:, 0])\n",
    "                ev_f.create_dataset('y', data=events[:, 1])\n",
    "                ev_f.create_dataset('t', data=events[:, 2])\n",
    "                ev_f.create_dataset('p', data=events[:, 3])\n",
    "            \n",
    "            # If you can, run this example on a GPU, it will be a lot faster.\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            img1_batch = preprocess(img1_batch).to(device)\n",
    "            img2_batch = preprocess(img2_batch).to(device)\n",
    "\n",
    "            model = raft_large(pretrained=True, progress=False).to(device)\n",
    "            model = model.eval()\n",
    "\n",
    "            list_of_flows = model(img1_batch.to(device), img2_batch.to(device))\n",
    "            predicted_flows = list_of_flows[-1]\n",
    "            # .flo 파일로 저장\n",
    "            write_flo(predicted_flows[0].permute(1, 2, 0), output_flow01_path)\n",
    "            list_of_flows = model(img2_batch.to(device), img1_batch.to(device))\n",
    "            predicted_flows = list_of_flows[-1]\n",
    "\n",
    "            # .flo 파일로 저장\n",
    "            write_flo(predicted_flows[0].permute(1, 2, 0), output_flow10_path)\n",
    "            \n",
    "#             break\n",
    "#         break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcei_flow",
   "language": "python",
   "name": "dcei_flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
